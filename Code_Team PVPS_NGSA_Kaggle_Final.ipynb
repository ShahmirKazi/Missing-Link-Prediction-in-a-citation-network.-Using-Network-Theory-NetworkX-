{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615512, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading training csv\n",
    "\n",
    "training = pd.read_csv('C:/Users/priya/Downloads/ngsa/training_set.txt', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>2000</td>\n",
       "      <td>questions in quantum physics</td>\n",
       "      <td>Rudolf Haag</td>\n",
       "      <td>NaN</td>\n",
       "      <td>an assessment of the present status of the the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>2000</td>\n",
       "      <td>topological defects in 3-d euclidean gravity</td>\n",
       "      <td>Sheng Li, Yong Zhang, Zhongyuan Zhu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>by making use of the complete decomposition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2000</td>\n",
       "      <td>n 0 supersymmetry and the non-relativistic mon...</td>\n",
       "      <td>Donald Spector</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we study some of the algebraic properties of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>2000</td>\n",
       "      <td>gluon pair production from space-time dependen...</td>\n",
       "      <td>Gouranga C. Nayak, Walter Greiner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we compute the probabilty for the processes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2000</td>\n",
       "      <td>instantons euclidean supersymmetry and wick ro...</td>\n",
       "      <td>A.V. Belitsky, S. V, oren, P. van Nieuwenhuizen</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>we discuss the reality properties of the fermi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>2000</td>\n",
       "      <td>noncommutativities of d-branes and theta chang...</td>\n",
       "      <td>Tsunehide Kuroki</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>in d-brane matrix models improved it is known ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>2000</td>\n",
       "      <td>boundary liouville field theory i boundary sta...</td>\n",
       "      <td>V. Fateev (Montpellier), A. Zamolodchikov (Rut...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>function montpellier liouville conformal field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>2000</td>\n",
       "      <td>gravity and the newtonian limit in the randall...</td>\n",
       "      <td>Rainer Dick, Dzo Mikulovicz</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>appended a remark on the compatibility of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>2000</td>\n",
       "      <td>gauge theories in local causal perturbation th...</td>\n",
       "      <td>Franz-Marc Boas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in this thesis quantum gauge theories are cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>2000</td>\n",
       "      <td>canonical quantization and topological theories</td>\n",
       "      <td>Alice Rogers</td>\n",
       "      <td>Nucl.Phys.Proc.Suppl.</td>\n",
       "      <td>dynamics and quantum gravity villasimius sardi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>2000</td>\n",
       "      <td>a comment on the holographic renormalization g...</td>\n",
       "      <td>Enrique Alvarez, Cesar Gomez</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>theorem the equivalence between the holographi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2000</td>\n",
       "      <td>the interaction of two hopf solitons</td>\n",
       "      <td>R. S. Ward</td>\n",
       "      <td>Phys.Lett.</td>\n",
       "      <td>this letter deals with topological solitons in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>2000</td>\n",
       "      <td>solitons in brane worlds ii</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nucl.Phys.</td>\n",
       "      <td>added minor errors corrected we study the solu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>2000</td>\n",
       "      <td>casimir energy of a dilute dielectric ball wit...</td>\n",
       "      <td>I. Klich, J. Feinberg, A. Mann, M. Revzen</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>light at finite temperature the casimir energy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2000</td>\n",
       "      <td>some uses of moduli spaces in particle and fie...</td>\n",
       "      <td>ST Tsou (Oxford)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>women in mathematics workshop on moduli spaces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>2000</td>\n",
       "      <td>a two-loop test of buscher's t-duality i</td>\n",
       "      <td>Zalan Horvath, Robert L. Karp, Laszlo Palla</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>we study the two loop quantum equivalence of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>2000</td>\n",
       "      <td>supersymmetry and bogomol'nyi equations in the...</td>\n",
       "      <td>Bogdan Damski (Jagiellonian University)</td>\n",
       "      <td>Acta</td>\n",
       "      <td>systems we take advantage of the superspace fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>2000</td>\n",
       "      <td>the string uncertainty relations follow from t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found.Phys.</td>\n",
       "      <td>principle polymomenta coordinates has been mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>2000</td>\n",
       "      <td>bps states of d 4 n 1 supersymmetry</td>\n",
       "      <td>Jerome P. Gauntlett, Gary W. Gibbons, Christop...</td>\n",
       "      <td>Commun.Math.Phys.</td>\n",
       "      <td>townsend expanded discussion on bps states in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2000</td>\n",
       "      <td>collapsing d-branes in calabi-yau moduli space</td>\n",
       "      <td>Brian R. Greene, C. I. Lazaroiu</td>\n",
       "      <td>Nucl.Phys.</td>\n",
       "      <td>we study the quantum volume of d-branes wrappe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2000</td>\n",
       "      <td>poisson-sigma models</td>\n",
       "      <td>Allen C. Hirshfeld (University of Dortmund), T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>university of dortmund physical variables in g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>2000</td>\n",
       "      <td>bi-local fields in noncommutative field theory</td>\n",
       "      <td>Satoshi Iso, Hikaru Kawai, Yoshihisa Kitazawa</td>\n",
       "      <td>Nucl.Phys.</td>\n",
       "      <td>we propose a bi-local representation in noncom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>2000</td>\n",
       "      <td>tree amplitudes and linearized susy invariants...</td>\n",
       "      <td>Domenico Seminara (LPT-ENS, Paris)</td>\n",
       "      <td>Nucl.Phys.Proc.Suppl.</td>\n",
       "      <td>gravity villasimius sept 13-17 1999 we exploit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>2000</td>\n",
       "      <td>type i and real algebraic geometry</td>\n",
       "      <td>F.A. Cachazo, C. Vafa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we revisit the duality between type i and hete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2000</td>\n",
       "      <td>a geometric discretisation scheme applied to t...</td>\n",
       "      <td>Samik Sen, Siddhartha Sen, James C. Sexton, Da...</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>theory we give a detailed general description ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_year                                              title  \\\n",
       "id                                                                  \n",
       "1001      2000              compactification geometry and duality   \n",
       "1002      2000  domain walls and massive gauged supergravity p...   \n",
       "1003      2000     comment on metric fluctuations in brane worlds   \n",
       "1004      2000         moving mirrors and thermodynamic paradoxes   \n",
       "1005      2000  bundles of chiral blocks and boundary conditio...   \n",
       "1006      2000                       questions in quantum physics   \n",
       "1007      2000       topological defects in 3-d euclidean gravity   \n",
       "1008      2000  n 0 supersymmetry and the non-relativistic mon...   \n",
       "1009      2000  gluon pair production from space-time dependen...   \n",
       "1010      2000  instantons euclidean supersymmetry and wick ro...   \n",
       "1011      2000  noncommutativities of d-branes and theta chang...   \n",
       "1012      2000  boundary liouville field theory i boundary sta...   \n",
       "1013      2000  gravity and the newtonian limit in the randall...   \n",
       "1014      2000  gauge theories in local causal perturbation th...   \n",
       "1015      2000    canonical quantization and topological theories   \n",
       "1016      2000  a comment on the holographic renormalization g...   \n",
       "1017      2000               the interaction of two hopf solitons   \n",
       "1018      2000                        solitons in brane worlds ii   \n",
       "1019      2000  casimir energy of a dilute dielectric ball wit...   \n",
       "1020      2000  some uses of moduli spaces in particle and fie...   \n",
       "1021      2000           a two-loop test of buscher's t-duality i   \n",
       "1022      2000  supersymmetry and bogomol'nyi equations in the...   \n",
       "1023      2000  the string uncertainty relations follow from t...   \n",
       "1024      2000                bps states of d 4 n 1 supersymmetry   \n",
       "1025      2000     collapsing d-branes in calabi-yau moduli space   \n",
       "1026      2000                               poisson-sigma models   \n",
       "1027      2000     bi-local fields in noncommutative field theory   \n",
       "1028      2000  tree amplitudes and linearized susy invariants...   \n",
       "1029      2000                 type i and real algebraic geometry   \n",
       "1030      2000  a geometric discretisation scheme applied to t...   \n",
       "\n",
       "                                                authors  \\\n",
       "id                                                        \n",
       "1001                                  Paul S. Aspinwall   \n",
       "1002                        M. Cvetic, H. Lu, C.N. Pope   \n",
       "1003                           Y.S. Myung, Gungwon Kang   \n",
       "1004                                     Adam D. Helfer   \n",
       "1005                            J. Fuchs, C. Schweigert   \n",
       "1006                                        Rudolf Haag   \n",
       "1007                Sheng Li, Yong Zhang, Zhongyuan Zhu   \n",
       "1008                                     Donald Spector   \n",
       "1009                  Gouranga C. Nayak, Walter Greiner   \n",
       "1010    A.V. Belitsky, S. V, oren, P. van Nieuwenhuizen   \n",
       "1011                                   Tsunehide Kuroki   \n",
       "1012  V. Fateev (Montpellier), A. Zamolodchikov (Rut...   \n",
       "1013                        Rainer Dick, Dzo Mikulovicz   \n",
       "1014                                    Franz-Marc Boas   \n",
       "1015                                       Alice Rogers   \n",
       "1016                       Enrique Alvarez, Cesar Gomez   \n",
       "1017                                         R. S. Ward   \n",
       "1018                                                NaN   \n",
       "1019          I. Klich, J. Feinberg, A. Mann, M. Revzen   \n",
       "1020                                   ST Tsou (Oxford)   \n",
       "1021        Zalan Horvath, Robert L. Karp, Laszlo Palla   \n",
       "1022            Bogdan Damski (Jagiellonian University)   \n",
       "1023                                                NaN   \n",
       "1024  Jerome P. Gauntlett, Gary W. Gibbons, Christop...   \n",
       "1025                    Brian R. Greene, C. I. Lazaroiu   \n",
       "1026  Allen C. Hirshfeld (University of Dortmund), T...   \n",
       "1027      Satoshi Iso, Hikaru Kawai, Yoshihisa Kitazawa   \n",
       "1028                 Domenico Seminara (LPT-ENS, Paris)   \n",
       "1029                              F.A. Cachazo, C. Vafa   \n",
       "1030  Samik Sen, Siddhartha Sen, James C. Sexton, Da...   \n",
       "\n",
       "               journal_name                                           abstract  \n",
       "id                                                                              \n",
       "1001                    NaN  these are notes based on lectures given at tas...  \n",
       "1002      Class.Quant.Grav.  we point out that massive gauged supergravity ...  \n",
       "1003                    NaN  recently ivanov and volovich hep-th 9912242 cl...  \n",
       "1004              Phys.Rev.  quantum fields responding to moving mirrors ha...  \n",
       "1005                    NaN  proceedings of lie iii clausthal july 1999 var...  \n",
       "1006                    NaN  an assessment of the present status of the the...  \n",
       "1007                    NaN  by making use of the complete decomposition of...  \n",
       "1008             Phys.Lett.  we study some of the algebraic properties of t...  \n",
       "1009                    NaN  we compute the probabilty for the processes a ...  \n",
       "1010             Phys.Lett.  we discuss the reality properties of the fermi...  \n",
       "1011             Phys.Lett.  in d-brane matrix models improved it is known ...  \n",
       "1012                    NaN  function montpellier liouville conformal field...  \n",
       "1013             Phys.Lett.  appended a remark on the compatibility of the ...  \n",
       "1014                    NaN  in this thesis quantum gauge theories are cons...  \n",
       "1015  Nucl.Phys.Proc.Suppl.  dynamics and quantum gravity villasimius sardi...  \n",
       "1016             Phys.Lett.  theorem the equivalence between the holographi...  \n",
       "1017             Phys.Lett.  this letter deals with topological solitons in...  \n",
       "1018             Nucl.Phys.  added minor errors corrected we study the solu...  \n",
       "1019              Phys.Rev.  light at finite temperature the casimir energy...  \n",
       "1020                    NaN  women in mathematics workshop on moduli spaces...  \n",
       "1021              Phys.Rev.  we study the two loop quantum equivalence of s...  \n",
       "1022                   Acta  systems we take advantage of the superspace fo...  \n",
       "1023            Found.Phys.  principle polymomenta coordinates has been mad...  \n",
       "1024      Commun.Math.Phys.  townsend expanded discussion on bps states in ...  \n",
       "1025             Nucl.Phys.  we study the quantum volume of d-branes wrappe...  \n",
       "1026                    NaN  university of dortmund physical variables in g...  \n",
       "1027             Nucl.Phys.  we propose a bi-local representation in noncom...  \n",
       "1028  Nucl.Phys.Proc.Suppl.  gravity villasimius sept 13-17 1999 we exploit...  \n",
       "1029                    NaN  we revisit the duality between type i and hete...  \n",
       "1030              Phys.Rev.  theory we give a detailed general description ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading node information data and naming the columns\n",
    "\n",
    "node_info = pd.read_csv('C:/Users/priya/Downloads/ngsa/node_info.csv', header= None)\n",
    "node_info.columns = ['id', 'pub_year', 'title', 'authors', 'journal_name', 'abstract']\n",
    "node_info = node_info.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 27770\n",
      "Number of edges : 335130\n"
     ]
    }
   ],
   "source": [
    "#creating the graph network - nodes and edges\n",
    "\n",
    "IDs = [node_id for node_id in node_info.index]\n",
    "\n",
    "training_list = training.values.tolist() # training dataframe convertion for easy edges list comprehension below\n",
    "edges = [(node_pair[0], node_pair[1]) for node_pair in training_list if node_pair[2] == 1]\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(IDs)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "print(\"Number of nodes : \" + str(G.number_of_nodes()))\n",
    "print(\"Number of edges : \" + str(G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43086"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subsetting the training set to facilitate computation on laptop\n",
    "\n",
    "training_reduced = training.sample(frac=0.07) # We keep 7%\n",
    "training_reduced.columns = ['source', 'target', 'Y']\n",
    "\n",
    "len(training_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Centrality features\n",
    "out_degree_centrality = nx.out_degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "total_centrality = nx.degree_centrality(G)\n",
    "training_reduced['source_out_centrality'] = training_reduced.apply(lambda row: out_degree_centrality[row.source],axis=1)\n",
    "training_reduced['target_in_centrality'] = training_reduced.apply(lambda row: in_degree_centrality[row.target],axis=1)\n",
    "training_reduced['source_centrality'] = training_reduced.apply(lambda row: total_centrality[row.source],axis=1)\n",
    "training_reduced['target_centrality'] = training_reduced.apply(lambda row: total_centrality[row.target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigen vector centrality\n",
    "eigen_centrality = nx.eigenvector_centrality(G)\n",
    "training_reduced['source_evc'] = training_reduced.apply(lambda row: eigen_centrality[row.source],axis=1)\n",
    "training_reduced['target_evc'] = training_reduced.apply(lambda row: eigen_centrality[row.target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page rank\n",
    "page_rank = nx.pagerank_scipy(G)\n",
    "training_reduced['target_pagerank'] = training_reduced.apply(lambda row: page_rank[row.target],axis=1)\n",
    "\n",
    "# Preferential Attachment\n",
    "# For a directed graph, is equal to K_out_source * K_in_target with K the number of neighbors. Which is equivalent to multiply the available centralities.\n",
    "training_reduced['preferencial_attachment'] = training_reduced.apply(lambda row: row.source_out_centrality * row.target_in_centrality,axis=1)\n",
    "\n",
    "# HITS algorithm\n",
    "hub_score, authority_score = nx.hits(G)\n",
    "training_reduced['source_hub_score'] = training_reduced.apply(lambda row: hub_score[row.source],axis=1)\n",
    "training_reduced['target_authority_score'] = training_reduced.apply(lambda row: authority_score[row.target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering on node attributes - based on node information like title, abstract, published date\n",
    "\n",
    "#difference in publication year\n",
    "\n",
    "training_reduced['pub_year_difference'] = training_reduced.apply(lambda row: node_info.pub_year[row.source] - node_info.pub_year[row.target] ,axis=1)\n",
    "training_reduced['pub_year_difference']=training_reduced['pub_year_difference'].where(training_reduced['pub_year_difference'] >= 0, -1)\n",
    "\n",
    "# common Authors\n",
    "node_info['authors'] = node_info['authors'].fillna(value='')\n",
    "training_reduced['common_authors'] = training_reduced.apply(lambda row: len(set(node_info.authors[row.source].split(\",\")).intersection(set(node_info.authors[row.target].split(\",\")))) ,axis=1)\n",
    "\n",
    "#number of common journal name\n",
    "node_info['journal_name'] = node_info['journal_name'].fillna(value='')\n",
    "training_reduced['common_journals'] = training_reduced.apply(lambda row: len(set(node_info.journal_name[row.source]).intersection(set(node_info.journal_name[row.target]))) ,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title similarity-spacy\n",
    "training_reduced['title_similarity'] = training_reduced.apply(lambda row: nlp(node_info.title[row.source]).similarity(nlp(node_info.title[row.target])) ,axis=1)\n",
    "\n",
    "# Abstract similarity- spacy\n",
    "training_reduced['abstract_similarity'] = training_reduced.apply(lambda row: nlp(node_info.abstract[row.source]).similarity(nlp(node_info.abstract[row.target])) ,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#cosine distance of abstracts - tf-idf\n",
    "\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "def tfidf_abstract():\n",
    "\n",
    "    tfidf_abstracts = []\n",
    "\n",
    "    for i in range(len(node_info)):\n",
    "        abstract = node_info.iloc[i]['abstract'].lower().split(\" \")\n",
    "        abstract = [token for token in abstract if token not in stpwds]\n",
    "        abstract = [stemmer.stem(token) for token in abstract]\n",
    "        tfidf_abstracts.append(\" \".join(abstract))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    tfidf_abstracts = vectorizer.fit_transform(tfidf_abstracts)\n",
    "\n",
    "    tfidf_abstracts = tfidf_abstracts.toarray()\n",
    "\n",
    "    return tfidf_abstracts\n",
    "\n",
    "tfidf_abstracts = tfidf_abstract()\n",
    "\n",
    "\n",
    "training_distance_abs = []\n",
    "    \n",
    "for i in range(len(training_reduced)):\n",
    "    source = training_reduced.iloc[i]['source']\n",
    "    target = training_reduced.iloc[i]['target']\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_info = tfidf_abstracts[index_source].reshape(1, -1)\n",
    "    target_info = tfidf_abstracts[index_target].reshape(1, -1)\n",
    "\n",
    "    training_distance_abs.append(pairwise_distances(source_info, target_info, metric='cosine', n_jobs=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_distance_abs2 = (np.asarray(training_distance_abs).reshape(len(training_reduced),1)).tolist()\n",
    "training_distance_abs = [val for sublist in training_distance_abs2 for val in sublist]\n",
    "training_reduced['training_dist_abs']=training_distance_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coside distance for titles - tf-idf\n",
    "\n",
    "def tfidf_title():\n",
    "\n",
    "    tfidf_title = []\n",
    "\n",
    "    for i in range(len(node_info)):\n",
    "        title = node_info.iloc[i]['title'].lower().split(\" \")\n",
    "        title = [token for token in title if token not in stpwds]\n",
    "        title = [stemmer.stem(token) for token in title]\n",
    "        tfidf_title.append(\" \".join(title))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    tfidf_title = vectorizer.fit_transform(tfidf_title)\n",
    "\n",
    "    tfidf_title = tfidf_title.toarray()\n",
    "\n",
    "    return tfidf_title\n",
    "\n",
    "tfidf_title = tfidf_title()\n",
    "\n",
    "\n",
    "training_distance_title = []\n",
    "    \n",
    "for i in range(len(training_reduced)):\n",
    "    source = training_reduced.iloc[i]['source']\n",
    "    target = training_reduced.iloc[i]['target']\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_info = tfidf_title[index_source].reshape(1, -1)\n",
    "    target_info = tfidf_title[index_target].reshape(1, -1)\n",
    "\n",
    "    training_distance_title.append(pairwise_distances(source_info, target_info, metric='cosine', n_jobs=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_distance_title2=(np.asarray(training_distance_title).reshape(len(training_reduced),1)).tolist()\n",
    "training_distance_title = [val for sublist in training_distance_title2 for val in sublist]\n",
    "training_reduced['training_dist_title']=training_distance_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>Y</th>\n",
       "      <th>source_out_centrality</th>\n",
       "      <th>target_in_centrality</th>\n",
       "      <th>source_centrality</th>\n",
       "      <th>target_centrality</th>\n",
       "      <th>source_evc</th>\n",
       "      <th>target_evc</th>\n",
       "      <th>target_pagerank</th>\n",
       "      <th>preferencial_attachment</th>\n",
       "      <th>source_hub_score</th>\n",
       "      <th>target_authority_score</th>\n",
       "      <th>pub_year_difference</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>common_journals</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>abstract_similarity</th>\n",
       "      <th>training_dist_abs</th>\n",
       "      <th>training_dist_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>507477</td>\n",
       "      <td>9705120</td>\n",
       "      <td>9608024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.014909</td>\n",
       "      <td>8.545371e-05</td>\n",
       "      <td>1.141593e-02</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>1.368923e-05</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.756032</td>\n",
       "      <td>0.948874</td>\n",
       "      <td>0.809550</td>\n",
       "      <td>0.919319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70689</td>\n",
       "      <td>12228</td>\n",
       "      <td>209062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>1.359490e-04</td>\n",
       "      <td>2.216651e-38</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492747</td>\n",
       "      <td>0.907499</td>\n",
       "      <td>0.944549</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422505</td>\n",
       "      <td>11282</td>\n",
       "      <td>5040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>1.799037e-10</td>\n",
       "      <td>2.722042e-08</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>9.596464e-06</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.220738</td>\n",
       "      <td>0.914158</td>\n",
       "      <td>0.755405</td>\n",
       "      <td>0.795546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379959</td>\n",
       "      <td>9610252</td>\n",
       "      <td>9606193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>2.683788e-05</td>\n",
       "      <td>5.874529e-04</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1.390190e-06</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.371616</td>\n",
       "      <td>0.938083</td>\n",
       "      <td>0.899424</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509173</td>\n",
       "      <td>101122</td>\n",
       "      <td>9410167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.026180</td>\n",
       "      <td>1.997202e-35</td>\n",
       "      <td>2.526680e-01</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>3.828989e-05</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.391077</td>\n",
       "      <td>0.978837</td>\n",
       "      <td>0.742181</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264424</td>\n",
       "      <td>101162</td>\n",
       "      <td>11002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>1.625915e-08</td>\n",
       "      <td>7.963036e-09</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>7.054698e-07</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.793785</td>\n",
       "      <td>0.952679</td>\n",
       "      <td>0.864674</td>\n",
       "      <td>0.892545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165444</td>\n",
       "      <td>9411242</td>\n",
       "      <td>9709101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>2.116976e-05</td>\n",
       "      <td>4.716742e-09</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>9.077736e-08</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.614609</td>\n",
       "      <td>0.963613</td>\n",
       "      <td>0.947215</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115845</td>\n",
       "      <td>205321</td>\n",
       "      <td>107028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>2.762823e-18</td>\n",
       "      <td>4.940902e-14</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5.511483e-07</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301396</td>\n",
       "      <td>0.924721</td>\n",
       "      <td>0.944288</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538105</td>\n",
       "      <td>3262</td>\n",
       "      <td>3184</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>2.604463e-09</td>\n",
       "      <td>9.312014e-10</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>4.188727e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.438315</td>\n",
       "      <td>0.952725</td>\n",
       "      <td>0.885956</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457209</td>\n",
       "      <td>9909178</td>\n",
       "      <td>9804176</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>1.136649e-12</td>\n",
       "      <td>8.132049e-07</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>7.236253e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.663370</td>\n",
       "      <td>0.935161</td>\n",
       "      <td>0.720701</td>\n",
       "      <td>0.778329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43086 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source   target  Y  source_out_centrality  target_in_centrality  \\\n",
       "507477  9705120  9608024  1               0.001044              0.013108   \n",
       "70689     12228   209062  0               0.000000              0.000000   \n",
       "422505    11282     5040  1               0.001801              0.005330   \n",
       "379959  9610252  9606193  1               0.000576              0.002413   \n",
       "509173   101122  9410167  1               0.001512              0.025316   \n",
       "...         ...      ... ..                    ...                   ...   \n",
       "264424   101162    11002  1               0.001152              0.000612   \n",
       "165444  9411242  9709101  0               0.000180              0.000504   \n",
       "115845   205321   107028  1               0.003061              0.000180   \n",
       "538105     3262     3184  1               0.000612              0.000684   \n",
       "457209  9909178  9804176  1               0.000648              0.001116   \n",
       "\n",
       "        source_centrality  target_centrality    source_evc    target_evc  \\\n",
       "507477           0.002809           0.014909  8.545371e-05  1.141593e-02   \n",
       "70689            0.000684           0.000252  1.359490e-04  2.216651e-38   \n",
       "422505           0.002521           0.005834  1.799037e-10  2.722042e-08   \n",
       "379959           0.001729           0.003349  2.683788e-05  5.874529e-04   \n",
       "509173           0.001548           0.026180  1.997202e-35  2.526680e-01   \n",
       "...                   ...                ...           ...           ...   \n",
       "264424           0.002377           0.001693  1.625915e-08  7.963036e-09   \n",
       "165444           0.000828           0.000720  2.116976e-05  4.716742e-09   \n",
       "115845           0.003817           0.000684  2.762823e-18  4.940902e-14   \n",
       "538105           0.000864           0.002485  2.604463e-09  9.312014e-10   \n",
       "457209           0.000720           0.002701  1.136649e-12  8.132049e-07   \n",
       "\n",
       "        target_pagerank  preferencial_attachment  source_hub_score  \\\n",
       "507477         0.001015             1.368923e-05          0.000103   \n",
       "70689          0.000011             0.000000e+00          0.000000   \n",
       "422505         0.000119             9.596464e-06          0.000181   \n",
       "379959         0.000069             1.390190e-06          0.000081   \n",
       "509173         0.003507             3.828989e-05          0.000098   \n",
       "...                 ...                      ...               ...   \n",
       "264424         0.000018             7.054698e-07          0.000025   \n",
       "165444         0.000025             9.077736e-08          0.000008   \n",
       "115845         0.000013             5.511483e-07          0.000165   \n",
       "538105         0.000018             4.188727e-07          0.000022   \n",
       "457209         0.000026             7.236253e-07          0.000022   \n",
       "\n",
       "        target_authority_score  pub_year_difference  common_authors  \\\n",
       "507477                0.001667                    1               0   \n",
       "70689                 0.000000                   -1               0   \n",
       "422505                0.000435                    0               0   \n",
       "379959                0.000276                    0               0   \n",
       "509173                0.003029                    7               0   \n",
       "...                        ...                  ...             ...   \n",
       "264424                0.000027                    1               0   \n",
       "165444                0.000009                   -1               0   \n",
       "115845                0.000036                    1               1   \n",
       "538105                0.000021                    0               0   \n",
       "457209                0.000091                    1               0   \n",
       "\n",
       "        common_journals  title_similarity  abstract_similarity  \\\n",
       "507477                9          0.756032             0.948874   \n",
       "70689                 0          0.492747             0.907499   \n",
       "422505                4          0.220738             0.914158   \n",
       "379959                5          0.371616             0.938083   \n",
       "509173                6          0.391077             0.978837   \n",
       "...                 ...               ...                  ...   \n",
       "264424                1          0.793785             0.952679   \n",
       "165444                5          0.614609             0.963613   \n",
       "115845                0          0.301396             0.924721   \n",
       "538105                8          0.438315             0.952725   \n",
       "457209                5          0.663370             0.935161   \n",
       "\n",
       "        training_dist_abs  training_dist_title  \n",
       "507477           0.809550             0.919319  \n",
       "70689            0.944549             1.000000  \n",
       "422505           0.755405             0.795546  \n",
       "379959           0.899424             1.000000  \n",
       "509173           0.742181             1.000000  \n",
       "...                   ...                  ...  \n",
       "264424           0.864674             0.892545  \n",
       "165444           0.947215             1.000000  \n",
       "115845           0.944288             1.000000  \n",
       "538105           0.885956             1.000000  \n",
       "457209           0.720701             0.778329  \n",
       "\n",
       "[43086 rows x 20 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "out = defaultdict(list)\n",
    "inc = defaultdict(list)\n",
    "for i, j in edges:\n",
    "    out[i].append(j)\n",
    "    inc[j].append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(source, target):\n",
    "    try:\n",
    "        denom = 1/len(set(out[source]) | set(inc[target]))\n",
    "    except:\n",
    "        denom = 1\n",
    "    jac = len(set(out[source]) & set(inc[target]))*denom\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comonneighbors(source, target):\n",
    "    source = set(out[source]) | set(inc[source])\n",
    "    target = set(out[target]) | set(inc[target])\n",
    "    return len(target & source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jaccard\n",
    "\n",
    "ls=[]\n",
    "for i in range(training_reduced.shape[0]):\n",
    "    ls.append(jaccard(training_reduced.iloc[i]['source'], training_reduced.iloc[i]['target']))\n",
    "training_reduced['jacard'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common neighbors\n",
    "\n",
    "ls = [] \n",
    "for i in range(training_reduced.shape[0]):\n",
    "    ls.append(comonneighbors(training_reduced.iloc[i]['source'], training_reduced.iloc[i]['target']))\n",
    "training_reduced['comonneigh'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in neighbors\n",
    "#out neighbors\n",
    "\n",
    "ls1=[]\n",
    "ls2=[]\n",
    "for i in range(training_reduced.shape[0]):\n",
    "    ls1.append(len(out[training_reduced.iloc[i]['source']]))\n",
    "    ls2.append(len(inc[training_reduced.iloc[i]['target']]))\n",
    "training_reduced['outneighbors'] = ls1\n",
    "training_reduced['inneighbors'] = ls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common successors\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    ls.append(len(set(out[rows['source']]) & set(out[rows['target']])))\n",
    "training_reduced['common_successors'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common predessesors\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    ls.append(len(set(inc[rows['source']]) & set(inc[rows['target']])))\n",
    "training_reduced['common_pred'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of overlapping words in title\n",
    "def overlapping_title(source, target):\n",
    "    title = node_info.loc[source, 'title']\n",
    "    title = [token for token in title.lower().split(\" \") if token not in stpwds]\n",
    "    source = [stemmer.stem(token) for token in title]\n",
    "    title = node_info.loc[target, 'title']\n",
    "    title = [token for token in title.lower().split(\" \") if token not in stpwds]\n",
    "    target = [stemmer.stem(token) for token in title]    \n",
    "    return len(set(target) & set(source))\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    ls.append(overlapping_title(rows['source'], rows['target']))\n",
    "\n",
    "training_reduced['overlap_title'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of overlapping words( >= 9 letters) in abstracts\n",
    "def overlapping_abstract(source, target):\n",
    "    abstract = node_info.loc[source, 'abstract']\n",
    "    abstract = [token for token in abstract.lower().split(\" \") if token not in stpwds and len(token)>8]\n",
    "    source = [stemmer.stem(token) for token in abstract]\n",
    "    abstract = node_info.loc[target, 'abstract']\n",
    "    abstract = [token for token in abstract.lower().split(\" \") if token not in stpwds and len(token)>8]\n",
    "    target = [stemmer.stem(token) for token in abstract]    \n",
    "    return len(set(target) & set(source))\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    ls.append(overlapping_abstract(rows['source'], rows['target']))\n",
    "\n",
    "training_reduced['overlap_abstract'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths of length one\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    try:\n",
    "        short_path = nx.shortest_path_length(G,source=rows['source'],target=rows['target'])\n",
    "    except:\n",
    "        short_path = -1\n",
    "    ls.append(short_path)\n",
    "training_reduced['short_path'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    ls.append(sum([len(inc[in_target]) for in_target in inc[rows['target']]]))\n",
    "training_reduced['popularity'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>Y</th>\n",
       "      <th>source_out_centrality</th>\n",
       "      <th>target_in_centrality</th>\n",
       "      <th>source_centrality</th>\n",
       "      <th>target_centrality</th>\n",
       "      <th>source_evc</th>\n",
       "      <th>target_evc</th>\n",
       "      <th>target_pagerank</th>\n",
       "      <th>...</th>\n",
       "      <th>jacard</th>\n",
       "      <th>comonneigh</th>\n",
       "      <th>outneighbors</th>\n",
       "      <th>inneighbors</th>\n",
       "      <th>common_successors</th>\n",
       "      <th>common_pred</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>short_path</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>507477</td>\n",
       "      <td>9705120</td>\n",
       "      <td>9608024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.014909</td>\n",
       "      <td>8.545371e-05</td>\n",
       "      <td>1.141593e-02</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70689</td>\n",
       "      <td>12228</td>\n",
       "      <td>209062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>1.359490e-04</td>\n",
       "      <td>2.216651e-38</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422505</td>\n",
       "      <td>11282</td>\n",
       "      <td>5040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>1.799037e-10</td>\n",
       "      <td>2.722042e-08</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379959</td>\n",
       "      <td>9610252</td>\n",
       "      <td>9606193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>2.683788e-05</td>\n",
       "      <td>5.874529e-04</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509173</td>\n",
       "      <td>101122</td>\n",
       "      <td>9410167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.026180</td>\n",
       "      <td>1.997202e-35</td>\n",
       "      <td>2.526680e-01</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017760</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>703</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264424</td>\n",
       "      <td>101162</td>\n",
       "      <td>11002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>1.625915e-08</td>\n",
       "      <td>7.963036e-09</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165444</td>\n",
       "      <td>9411242</td>\n",
       "      <td>9709101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>2.116976e-05</td>\n",
       "      <td>4.716742e-09</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115845</td>\n",
       "      <td>205321</td>\n",
       "      <td>107028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>2.762823e-18</td>\n",
       "      <td>4.940902e-14</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538105</td>\n",
       "      <td>3262</td>\n",
       "      <td>3184</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>2.604463e-09</td>\n",
       "      <td>9.312014e-10</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457209</td>\n",
       "      <td>9909178</td>\n",
       "      <td>9804176</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>1.136649e-12</td>\n",
       "      <td>8.132049e-07</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43086 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source   target  Y  source_out_centrality  target_in_centrality  \\\n",
       "507477  9705120  9608024  1               0.001044              0.013108   \n",
       "70689     12228   209062  0               0.000000              0.000000   \n",
       "422505    11282     5040  1               0.001801              0.005330   \n",
       "379959  9610252  9606193  1               0.000576              0.002413   \n",
       "509173   101122  9410167  1               0.001512              0.025316   \n",
       "...         ...      ... ..                    ...                   ...   \n",
       "264424   101162    11002  1               0.001152              0.000612   \n",
       "165444  9411242  9709101  0               0.000180              0.000504   \n",
       "115845   205321   107028  1               0.003061              0.000180   \n",
       "538105     3262     3184  1               0.000612              0.000684   \n",
       "457209  9909178  9804176  1               0.000648              0.001116   \n",
       "\n",
       "        source_centrality  target_centrality    source_evc    target_evc  \\\n",
       "507477           0.002809           0.014909  8.545371e-05  1.141593e-02   \n",
       "70689            0.000684           0.000252  1.359490e-04  2.216651e-38   \n",
       "422505           0.002521           0.005834  1.799037e-10  2.722042e-08   \n",
       "379959           0.001729           0.003349  2.683788e-05  5.874529e-04   \n",
       "509173           0.001548           0.026180  1.997202e-35  2.526680e-01   \n",
       "...                   ...                ...           ...           ...   \n",
       "264424           0.002377           0.001693  1.625915e-08  7.963036e-09   \n",
       "165444           0.000828           0.000720  2.116976e-05  4.716742e-09   \n",
       "115845           0.003817           0.000684  2.762823e-18  4.940902e-14   \n",
       "538105           0.000864           0.002485  2.604463e-09  9.312014e-10   \n",
       "457209           0.000720           0.002701  1.136649e-12  8.132049e-07   \n",
       "\n",
       "        target_pagerank  ...    jacard  comonneigh  outneighbors  inneighbors  \\\n",
       "507477         0.001015  ...  0.031496          30            29          364   \n",
       "70689          0.000011  ...  0.000000           0             0            0   \n",
       "422505         0.000119  ...  0.076087          22            50          148   \n",
       "379959         0.000069  ...  0.012195           4            16           67   \n",
       "509173         0.003507  ...  0.017760          14            42          703   \n",
       "...                 ...  ...       ...         ...           ...          ...   \n",
       "264424         0.000018  ...  0.020833          21            32           17   \n",
       "165444         0.000025  ...  0.000000           0             5           14   \n",
       "115845         0.000013  ...  0.022727          10            85            5   \n",
       "538105         0.000018  ...  0.000000           9            17           19   \n",
       "457209         0.000026  ...  0.065217          10            18           31   \n",
       "\n",
       "        common_successors  common_pred  overlap_title  overlap_abstract  \\\n",
       "507477                  1           17              1                 5   \n",
       "70689                   0            0              0                 2   \n",
       "422505                  0            9              1                 1   \n",
       "379959                  3            0              0                 2   \n",
       "509173                  1            0              0                 2   \n",
       "...                   ...          ...            ...               ...   \n",
       "264424                 18            2              1                 0   \n",
       "165444                  0            0              0                 2   \n",
       "115845                  8            0              0                 1   \n",
       "538105                  6            3              0                 0   \n",
       "457209                  6            1              1                 3   \n",
       "\n",
       "        short_path  popularity  \n",
       "507477           1       15075  \n",
       "70689           -1           0  \n",
       "422505           1        2771  \n",
       "379959           1        1072  \n",
       "509173           1       28479  \n",
       "...            ...         ...  \n",
       "264424           1         399  \n",
       "165444          -1          72  \n",
       "115845           1          42  \n",
       "538105           1         136  \n",
       "457209           1         346  \n",
       "\n",
       "[43086 rows x 30 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv('C:/Users/priya/Downloads/ngsa/testing_set.txt', sep = ' ', header = None)\n",
    "testing.columns = ['source', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Centrality features\n",
    "out_degree_centrality = nx.out_degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "total_centrality = nx.degree_centrality(G)\n",
    "testing['source_out_centrality'] = testing.apply(lambda row: out_degree_centrality[row.source],axis=1)\n",
    "testing['target_in_centrality'] = testing.apply(lambda row: in_degree_centrality[row.target],axis=1)\n",
    "testing['source_centrality'] = testing.apply(lambda row: total_centrality[row.source],axis=1)\n",
    "testing['target_centrality'] = testing.apply(lambda row: total_centrality[row.target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigen vector centrality\n",
    "eigen_centrality = nx.eigenvector_centrality(G)\n",
    "testing['source_evc'] = testing.apply(lambda row: eigen_centrality[row.source],axis=1)\n",
    "testing['target_evc'] = testing.apply(lambda row: eigen_centrality[row.target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page rank\n",
    "page_rank = nx.pagerank_scipy(G)\n",
    "testing['target_pagerank'] = testing.apply(lambda row: page_rank[row.target],axis=1)\n",
    "\n",
    "# Preferential Attachment\n",
    "# For a directed graph, is equal to K_out_source * K_in_target with K the number of neighbors. Which is equivalent to multiply the available centralities.\n",
    "testing['preferencial_attachment'] = testing.apply(lambda row: row.source_out_centrality * row.target_in_centrality,axis=1)\n",
    "\n",
    "# HITS algorithm\n",
    "hub_score, authority_score = nx.hits(G)\n",
    "testing['source_hub_score'] = testing.apply(lambda row: hub_score[row.source],axis=1)\n",
    "testing['target_authority_score'] = testing.apply(lambda row: authority_score[row.target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering on node attributes - based on node information like title, abstract, published date\n",
    "\n",
    "#difference in publication year\n",
    "\n",
    "testing['pub_year_difference'] = testing.apply(lambda row: node_info.pub_year[row.source] - node_info.pub_year[row.target] ,axis=1)\n",
    "testing['pub_year_difference']=testing['pub_year_difference'].where(testing['pub_year_difference'] >= 0, -1)\n",
    "\n",
    "# common Authors\n",
    "node_info['authors'] = node_info['authors'].fillna(value='')\n",
    "testing['common_authors'] = testing.apply(lambda row: len(set(node_info.authors[row.source].split(\",\")).intersection(set(node_info.authors[row.target].split(\",\")))) ,axis=1)\n",
    "\n",
    "#number of common journal name\n",
    "node_info['journal_name'] = node_info['journal_name'].fillna(value='')\n",
    "testing['common_journals'] = testing.apply(lambda row: len(set(node_info.journal_name[row.source]).intersection(set(node_info.journal_name[row.target]))) ,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "D:\\anaconda\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Title\n",
    "testing['title_similarity'] = testing.apply(lambda row: nlp(node_info.title[row.source]).similarity(nlp(node_info.title[row.target])) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract similarity- spacy\n",
    "testing['abstract_similarity'] = testing.apply(lambda row: nlp(node_info.abstract[row.source]).similarity(nlp(node_info.abstract[row.target])) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine distance - abstract\n",
    "testing_distance_abs = []\n",
    "    \n",
    "for i in range(len(testing)):\n",
    "    source = testing.iloc[i]['source']\n",
    "    target = testing.iloc[i]['target']\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_info = tfidf_abstracts[index_source].reshape(1, -1)\n",
    "    target_info = tfidf_abstracts[index_target].reshape(1, -1)\n",
    "\n",
    "    testing_distance_abs.append(pairwise_distances(source_info, target_info, metric='cosine', n_jobs=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_distance_abs2 = (np.asarray(testing_distance_abs).reshape(len(testing),1)).tolist()\n",
    "testing_distance_abs = [val for sublist in testing_distance_abs2 for val in sublist]\n",
    "testing['testing_dist_abs']=testing_distance_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine distance- title\n",
    "\n",
    "testing_distance_title = []\n",
    "    \n",
    "for i in range(len(testing)):\n",
    "    source = testing.iloc[i]['source']\n",
    "    target = testing.iloc[i]['target']\n",
    "\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    source_info = tfidf_title[index_source].reshape(1, -1)\n",
    "    target_info = tfidf_title[index_target].reshape(1, -1)\n",
    "\n",
    "    testing_distance_title.append(pairwise_distances(source_info, target_info, metric='cosine', n_jobs=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_distance_title2=(np.asarray(testing_distance_title).reshape(len(testing),1)).tolist()\n",
    "testing_distance_title = [val for sublist in testing_distance_title2 for val in sublist]\n",
    "testing['training_dist_title']=testing_distance_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jaccard\n",
    "\n",
    "ls=[]\n",
    "for i in range(testing.shape[0]):\n",
    "    ls.append(jaccard(testing.iloc[i]['source'], testing.iloc[i]['target']))\n",
    "testing['jacard'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common neighbors\n",
    "\n",
    "ls = [] \n",
    "for i in range(testing.shape[0]):\n",
    "    ls.append(comonneighbors(testing.iloc[i]['source'], testing.iloc[i]['target']))\n",
    "testing['comonneigh'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in neighbors\n",
    "#out neighbors\n",
    "\n",
    "ls1=[]\n",
    "ls2=[]\n",
    "for i in range(testing.shape[0]):\n",
    "    ls1.append(len(out[testing.iloc[i]['source']]))\n",
    "    ls2.append(len(inc[testing.iloc[i]['target']]))\n",
    "testing['outneighbors'] = ls1\n",
    "testing['inneighbors'] = ls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common successors\n",
    "ls=[]\n",
    "for i, rows in testing.iterrows():\n",
    "    ls.append(len(set(out[rows['source']]) & set(out[rows['target']])))\n",
    "testing['common_successors'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common predessesors\n",
    "ls=[]\n",
    "for i, rows in testing.iterrows():\n",
    "    ls.append(len(set(inc[rows['source']]) & set(inc[rows['target']])))\n",
    "testing['common_pred'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of overlapping words in title\n",
    "ls=[]\n",
    "for i, rows in testing.iterrows():\n",
    "    ls.append(overlapping_title(rows['source'], rows['target']))\n",
    "\n",
    "testing['overlap_title'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of overlapping words( >= 9 letters) in abstracts\n",
    "ls=[]\n",
    "for i, rows in testing.iterrows():\n",
    "    ls.append(overlapping_abstract(rows['source'], rows['target']))\n",
    "\n",
    "testing['overlap_abstract'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths of length onele]\n",
    "ls=[]\n",
    "for i, rows in training_reduced.iterrows():\n",
    "    try:    \n",
    "        short_path = nx.shortest_path_length(G,source=rows['source'],target=rows['target'])\n",
    "    except:\n",
    "        short_path = -1\n",
    "    if short_path == 1:\n",
    "        G.remove_edge(rows['source'],rows['target'])\n",
    "        try:    \n",
    "            short_path = nx.shortest_path_length(G,source=rows['source'],target=rows['target'])\n",
    "        except:\n",
    "            short_path = -1\n",
    "        G.add_edge(rows['source'],rows['target'])\n",
    "    ls.append(short_path)\n",
    "training_reduced['short_path'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity\n",
    "ls=[]\n",
    "for i, rows in testing.iterrows():\n",
    "    ls.append(sum([len(inc[in_target]) for in_target in inc[rows['target']]]))\n",
    "testing['popularity'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_out_centrality</th>\n",
       "      <th>target_in_centrality</th>\n",
       "      <th>source_centrality</th>\n",
       "      <th>target_centrality</th>\n",
       "      <th>source_evc</th>\n",
       "      <th>target_evc</th>\n",
       "      <th>target_pagerank</th>\n",
       "      <th>preferencial_attachment</th>\n",
       "      <th>...</th>\n",
       "      <th>jacard</th>\n",
       "      <th>comonneigh</th>\n",
       "      <th>outneighbors</th>\n",
       "      <th>inneighbors</th>\n",
       "      <th>common_successors</th>\n",
       "      <th>common_pred</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>short_path</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>9.082262e-06</td>\n",
       "      <td>5.472969e-14</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>3.890458e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>2.993220e-11</td>\n",
       "      <td>3.777396e-07</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1.026692e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075556</td>\n",
       "      <td>24</td>\n",
       "      <td>203</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.026612</td>\n",
       "      <td>4.259980e-03</td>\n",
       "      <td>8.746000e-02</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>1.318087e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>726</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1.489686e-10</td>\n",
       "      <td>1.513932e-10</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>8.299645e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>21</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>1.032959e-35</td>\n",
       "      <td>5.801489e-02</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.120452e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32643</td>\n",
       "      <td>9705209</td>\n",
       "      <td>9305083</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>1.494568e-06</td>\n",
       "      <td>2.939870e-02</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>2.316120e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32644</td>\n",
       "      <td>9307023</td>\n",
       "      <td>9503118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>4.822759e-07</td>\n",
       "      <td>1.318309e-08</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32645</td>\n",
       "      <td>9608095</td>\n",
       "      <td>9205058</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>5.156258e-05</td>\n",
       "      <td>5.108872e-03</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>6.069115e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32646</td>\n",
       "      <td>9407008</td>\n",
       "      <td>106256</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>5.007298e-03</td>\n",
       "      <td>4.114137e-12</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.167138e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32647</td>\n",
       "      <td>208144</td>\n",
       "      <td>7142</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>1.032959e-35</td>\n",
       "      <td>3.481527e-10</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.945229e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32648 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source   target  source_out_centrality  target_in_centrality  \\\n",
       "0      9807076  9807139               0.000360              0.000108   \n",
       "1       109162     1182               0.007310              0.001404   \n",
       "2      9702187  9510135               0.000504              0.026144   \n",
       "3       111048   110115               0.001440              0.000576   \n",
       "4      9910176  9410073               0.000216              0.005186   \n",
       "...        ...      ...                    ...                   ...   \n",
       "32643  9705209  9305083               0.001368              0.001693   \n",
       "32644  9307023  9503118               0.000000              0.000216   \n",
       "32645  9608095  9205058               0.000936              0.000648   \n",
       "32646  9407008   106256               0.000216              0.000540   \n",
       "32647   208144     7142               0.001080              0.000180   \n",
       "\n",
       "       source_centrality  target_centrality    source_evc    target_evc  \\\n",
       "0               0.002125           0.000648  9.082262e-06  5.472969e-14   \n",
       "1               0.010911           0.001621  2.993220e-11  3.777396e-07   \n",
       "2               0.008031           0.026612  4.259980e-03  8.746000e-02   \n",
       "3               0.001837           0.002341  1.489686e-10  1.513932e-10   \n",
       "4               0.000252           0.005402  1.032959e-35  5.801489e-02   \n",
       "...                  ...                ...           ...           ...   \n",
       "32643           0.001512           0.001873  1.494568e-06  2.939870e-02   \n",
       "32644           0.000072           0.000432  4.822759e-07  1.318309e-08   \n",
       "32645           0.001404           0.000648  5.156258e-05  5.108872e-03   \n",
       "32646           0.000828           0.001404  5.007298e-03  4.114137e-12   \n",
       "32647           0.001116           0.000612  1.032959e-35  3.481527e-10   \n",
       "\n",
       "       target_pagerank  preferencial_attachment  ...    jacard  comonneigh  \\\n",
       "0             0.000012             3.890458e-08  ...  0.000000           0   \n",
       "1             0.000069             1.026692e-05  ...  0.075556          24   \n",
       "2             0.002588             1.318087e-05  ...  0.006803          59   \n",
       "3             0.000016             8.299645e-07  ...  0.056604          21   \n",
       "4             0.000879             1.120452e-06  ...  0.000000           0   \n",
       "...                ...                      ...  ...       ...         ...   \n",
       "32643         0.000367             2.316120e-06  ...  0.011905           1   \n",
       "32644         0.000019             0.000000e+00  ...  0.000000           0   \n",
       "32645         0.000110             6.069115e-07  ...  0.000000           0   \n",
       "32646         0.000026             1.167138e-07  ...  0.000000           0   \n",
       "32647         0.000015             1.945229e-07  ...  0.029412           1   \n",
       "\n",
       "       outneighbors  inneighbors  common_successors  common_pred  \\\n",
       "0                10            3                  0            0   \n",
       "1               203           39                  6            1   \n",
       "2                14          726                  0           54   \n",
       "3                40           16                 14            4   \n",
       "4                 6          144                  0            0   \n",
       "...             ...          ...                ...          ...   \n",
       "32643            38           47                  0            0   \n",
       "32644             0            6                  0            0   \n",
       "32645            26           18                  0            0   \n",
       "32646             6           15                  0            0   \n",
       "32647            30            5                  0            0   \n",
       "\n",
       "       overlap_title  overlap_abstract  short_path  popularity  \n",
       "0                  0                 1          16           5  \n",
       "1                  2                 3           2        1266  \n",
       "2                  1                 0           2       28210  \n",
       "3                  1                 6           2         214  \n",
       "4                  0                 1           3        8521  \n",
       "...              ...               ...         ...         ...  \n",
       "32643              3                 0           2        1789  \n",
       "32644              0                 0          -1           9  \n",
       "32645              0                 2           3         324  \n",
       "32646              0                 0          -1          62  \n",
       "32647              0                 2           2          24  \n",
       "\n",
       "[32648 rows x 29 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutting out few features for trial\n",
    "\n",
    "x_train_cut= training_reduced.drop(['common_journals','source_centrality', 'target_centrality', 'source_evc','target_evc','short_path'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutting out few features from test for trial\n",
    "\n",
    "x_test_cut=testing.drop(['common_journals','source_centrality', 'target_centrality', 'source_evc','target_evc','short_path'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling all the features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "training_reduced_scale= scaler.fit_transform(training_reduced[['source_out_centrality','target_in_centrality','source_centrality','target_centrality','source_evc','target_evc','target_pagerank','preferencial_attachment','source_hub_score','target_authority_score', 'pub_year_difference','common_authors','common_journals','title_similarity','abstract_similarity','training_dist_abs','training_dist_title','jacard','comonneigh','outneighbors','inneighbors','common_successors','common_pred','overlap_title','overlap_abstract','popularity']])\n",
    "training_reduced_scale = pd.DataFrame(training_reduced_scale)\n",
    "\n",
    "testing_scale=scaler.fit_transform(testing[['source_out_centrality','target_in_centrality','source_centrality','target_centrality','source_evc','target_evc','target_pagerank','preferencial_attachment','source_hub_score','target_authority_score', 'pub_year_difference','common_authors','common_journals','title_similarity','abstract_similarity','testing_dist_abs','training_dist_title','jacard','comonneigh','outneighbors','inneighbors','common_successors','common_pred','overlap_title','overlap_abstract','popularity']])\n",
    "testing_scale = pd.DataFrame(testing_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split for actual(unscaled) features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_cut.drop(['source', 'target', 'Y'], axis= 1), training_reduced.Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train split for scaled features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_reduced_scale, training_reduced.Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "RF =  RandomForestClassifier(n_estimators= 1000)\n",
    "RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755163611046647"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  4.9min finished\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter grid search for randomforest\n",
    "n_estimators = [200, 400, 600, 800, 1000]\n",
    "max_depth = [5, 10, 15]\n",
    "min_samples_split = [2,4,6]\n",
    "min_samples_leaf = [1, 3, 6]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, \n",
    "              min_samples_split = min_samples_split,\n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "clf = GridSearchCV(RandomForestClassifier(), hyperF, cv = 3, verbose = 1,\n",
    "                      n_jobs = -1, scoring='accuracy')\n",
    "\n",
    "clf.fit(X_test, y_test)\n",
    "clf_best = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.941749825945695"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb= XGBClassifier(n_estimators=1000)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747041076815967"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt= DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543977721048967"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc= SVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949872360176375"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc= LinearSVC()\n",
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9554420979345556"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et=ExtraTreesClassifier(n_estimators=1000)\n",
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747041076815967"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754003249013692"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.8,\n",
       "                   n_estimators=1000, random_state=None)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab=AdaBoostClassifier(n_estimators=1000, learning_rate=0.8)\n",
    "ab.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720352750058018"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=None,\n",
       "                   estimators=[('lr',\n",
       "                                LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                   dual=False,\n",
       "                                                   fit_intercept=True,\n",
       "                                                   intercept_scaling=1,\n",
       "                                                   l1_ratio=None, max_iter=100,\n",
       "                                                   multi_class='auto',\n",
       "                                                   n_jobs=None, penalty='l2',\n",
       "                                                   random_state=None,\n",
       "                                                   solver='lbfgs', tol=0.0001,\n",
       "                                                   verbose=0,\n",
       "                                                   warm_start=False)),\n",
       "                               ('xgb',\n",
       "                                XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                              colsample_bylev...\n",
       "                                                       n_estimators=1000,\n",
       "                                                       n_jobs=None,\n",
       "                                                       oob_score=False,\n",
       "                                                       random_state=None,\n",
       "                                                       verbose=0,\n",
       "                                                       warm_start=False))],\n",
       "                   final_estimator=LinearSVC(C=1.0, class_weight=None,\n",
       "                                             dual=True, fit_intercept=True,\n",
       "                                             intercept_scaling=1,\n",
       "                                             loss='squared_hinge',\n",
       "                                             max_iter=1000, multi_class='ovr',\n",
       "                                             penalty='l2', random_state=None,\n",
       "                                             tol=0.0001, verbose=0),\n",
       "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stacking classifiers\n",
    "\n",
    "base_learners = [\n",
    "                 ('lr', LogisticRegression()),\n",
    "                 ('xgb', XGBClassifier(n_estimators=1000)),\n",
    "                 ('rf', RandomForestClassifier(n_estimators=1000))\n",
    "                     \n",
    "                ]\n",
    "stc = StackingClassifier(estimators=base_learners, final_estimator=LinearSVC())\n",
    "stc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774889765606869"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing predictions to csv\n",
    "\n",
    "predictions = list(clf_best.predict(testing_scale))\n",
    "pred_df = pd.DataFrame(predictions,columns =['category'])\n",
    "pred_df.index.names = ['id']\n",
    "pred_df.to_csv('C:/Users/priya/Downloads/ngsa/predictions_RFgridsearch_fullfeatures_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
